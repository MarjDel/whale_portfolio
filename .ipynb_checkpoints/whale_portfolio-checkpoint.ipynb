{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f351633",
   "metadata": {},
   "source": [
    " #  A Whale off the Port(folio)\r\n",
    " ---\r\n",
    "\r\n",
    " In this assignment, you'll get to use what you've learned this week to evaluate the performance among various algorithmic, hedge, and mutual fund portfolios and compare them against the S&P 500 Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Imports\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import datetime as dt\r\n",
    "from pathlib import Path\r\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f7e97",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "In this section, you will need to read the CSV files into DataFrames and perform any necessary data cleaning steps. After cleaning, combine all DataFrames into a single DataFrame.\n",
    "\n",
    "Files:\n",
    "\n",
    "* `whale_returns.csv`: Contains returns of some famous \"whale\" investors' portfolios.\n",
    "\n",
    "* `algo_returns.csv`: Contains returns from the in-house trading algorithms from Harold's company.\n",
    "\n",
    "* `sp500_history.csv`: Contains historical closing prices of the S&P 500 Index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db651d",
   "metadata": {},
   "source": [
    "## Whale Returns\n",
    "\n",
    "Read the Whale Portfolio daily returns and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b24628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading whale returns\r\n",
    "#create a path to the whale returns file\r\n",
    "whale_returns_csv = Path(\"../resources/whale_returns.csv\")\r\n",
    "# read the whale returns csv data\r\n",
    "whale_returns_df = pd.read_csv(whale_returns_csv, index_col = \"Date\", infer_datetime_format = True, parse_dates = True).sort_index(ascending = True)\r\n",
    "\r\n",
    "# whale_returns_df = pd.read_csv(whale_returns_csv)\r\n",
    "whale_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls\r\n",
    "# Identify number of nulls\r\n",
    "whale_returns_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls\r\n",
    "# drop null records & validate nulls have been dropped\r\n",
    "whale_returns = whale_returns_df.dropna()\r\n",
    "whale_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2858b8c",
   "metadata": {},
   "source": [
    "## Algorithmic Daily Returns\n",
    "\n",
    "Read the algorithmic daily returns and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ecea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading algorithmic returns\r\n",
    "#create a path to the file\r\n",
    "algo_returns_csv = Path(\"../resources/algo_returns.csv\")\r\n",
    "\r\n",
    "# read the algo returns csv data, then sort data\r\n",
    "algo_returns_df = pd.read_csv(algo_returns_csv, index_col = \"Date\", infer_datetime_format = True, parse_dates = True).sort_index(ascending = True)\r\n",
    "algo_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffe1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls\r\n",
    "# Identify number of nulls\r\n",
    "algo_returns_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba67444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls\r\n",
    "# drop null records \r\n",
    "algo_returns_df = algo_returns_df.dropna()\r\n",
    "algo_returns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b60b24",
   "metadata": {},
   "source": [
    "## S&P 500 Returns\n",
    "\n",
    "Read the S&P 500 historic closing prices and create a new daily returns DataFrame from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492bd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading S&P 500 Closing Prices\r\n",
    "#create a path to the sp500 history file\r\n",
    "sp500_history_csv = Path(\"../resources/sp500_history.csv\")\r\n",
    "# read the sp500 history csv data, then sort \r\n",
    "sp500_history_df = pd.read_csv(sp500_history_csv, index_col = \"Date\", infer_datetime_format = True, parse_dates = True).sort_index(ascending=True)\r\n",
    "sp500_history_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ebfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Data Types\r\n",
    "# Cast Close series as float\r\n",
    "sp500_history_df[\"Close\"] = sp500_history_df[\"Close\"].astype(\"float\")\r\n",
    "sp500_history_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Data Types\r\n",
    "sp500_return = sp500_history_df.pct_change()\r\n",
    "sp500_return.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d527827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Daily Returns\r\n",
    "spdaily_returns = sp500_history_df.pct_change()\r\n",
    "spdaily_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53502c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls\r\n",
    "spdaily_returns=spdaily_returns.dropna()\r\n",
    "spdaily_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f39a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Column\r\n",
    "sp500_return.rename(columns={\"Close\":\"S&P 500\"}, inplace=True)\r\n",
    "sp500_return.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f91bf",
   "metadata": {},
   "source": [
    "## Combine Whale, Algorithmic, and S&P 500 Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames into a single DataFrame\r\n",
    "\r\n",
    "combine_df = pd.concat([whale_returns_df, spdaily_returns, algo_returns_df], axis=\"columns\", join=\"inner\")\r\n",
    "combine_df=combine_df.dropna()\r\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72a138",
   "metadata": {},
   "source": [
    "# Conduct Quantitative Analysis\n",
    "\n",
    "In this section, you will calculate and visualize performance and risk metrics for the portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0912f53",
   "metadata": {},
   "source": [
    "## Performance Anlysis\n",
    "\n",
    "#### Calculate and Plot the daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa91d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily returns\r\n",
    "combine_df.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2618cc",
   "metadata": {},
   "source": [
    "#### Calculate and Plot cumulative returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative returns\r\n",
    "cumulative_returns = (combine_df).cumprod() \r\n",
    "cumulative_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns\r\n",
    "cumulative_returns.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2e516",
   "metadata": {},
   "source": [
    "## Risk Analysis\n",
    "\n",
    "Determine the _risk_ of each portfolio:\n",
    "\n",
    "1. Create a box plot for each portfolio. \n",
    "2. Calculate the standard deviation for all portfolios\n",
    "4. Determine which portfolios are riskier than the S&P 500\n",
    "5. Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3270fd7",
   "metadata": {},
   "source": [
    "### Create a box plot for each portfolio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ca2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot to visually show risk\r\n",
    "cumulative_returns.plot.box(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e5201",
   "metadata": {},
   "source": [
    "### Calculate Standard Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a180f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Standard Deviations\r\n",
    "# Calculate the standard deviation for each portfolio. \r\n",
    "standard_deviation_df = combine_df.std()\r\n",
    "standard_deviation_df.head()\r\n",
    "# Which portfolios are riskier than the S&P 500?\r\n",
    "risky_df = standard_deviation_df[standard_deviation_df > standard_deviation_df[\"Close\"]]\r\n",
    "risky_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0817a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (standard_deviation_df[\"Close\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077bb53",
   "metadata": {},
   "source": [
    "### Determine which portfolios are riskier than the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfd1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which portfolios are riskier than the S&P 500\r\n",
    "risky_df = standard_deviation_df[standard_deviation_df > standard_deviation_df[\"Close\"]]\r\n",
    "risky_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7b33c",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8690a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized standard deviation (252 trading days)\r\n",
    "standard_deviation_df * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ea7cb",
   "metadata": {},
   "source": [
    "## Rolling Statistics\n",
    "\n",
    "Risk changes over time. Analyze the rolling statistics for Risk and Beta. \n",
    "\n",
    "1. Calculate and plot the rolling standard deviation for all portfolios using a 21-day window\n",
    "2. Calculate the correlation between each stock to determine which portfolios may mimick the S&P 500\n",
    "3. Choose one portfolio, then calculate and plot the 60-day rolling beta between it and the S&P 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a373b9",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` for all portfolios with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the rolling standard deviation for\r\n",
    "# the S&P 500 and whale portfolios using a 21 trading day window\r\n",
    "combined_df = pd.concat([whale_returns_df, spdaily_returns], axis=\"columns\", join=\"inner\")\r\n",
    "combine_df = combine_df.dropna()\r\n",
    "rolling_df = combine_df.rolling(window=21).std()\r\n",
    "rolling_df.dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6810cdb",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df026e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation\r\n",
    "# Display the correlation matrix\r\n",
    "rolling_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ab6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the correlation\r\n",
    "rolling_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990042a",
   "metadata": {},
   "source": [
    "### Calculate and Plot Beta for a chosen portfolio and the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Beta for a single portfolio compared to the total market (S&P 500)\r\n",
    "# Calculate covariance of a single portfolio\r\n",
    "# Calculate variance of S&P 500\r\n",
    "# Computing beta\r\n",
    "covariance = combine_df['SOROS FUND MANAGEMENT LLC'].cov(combine_df['Close'])\r\n",
    "variance = combine_df['SOROS FUND MANAGEMENT LLC'].var()\r\n",
    "SOROS_beta = covariance / variance\r\n",
    "SOROS_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot beta trend\r\n",
    "SOROS_beta.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa49a302",
   "metadata": {},
   "source": [
    "## Rolling Statistics Challenge: Exponentially Weighted Average \n",
    "\n",
    "An alternative way to calculate a rolling window is to take the exponentially weighted moving average. This is like a moving window average, but it assigns greater importance to more recent observations. Try calculating the [`ewm`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html) with a 21-day half-life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fcebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a rolling window using the exponentially weighted moving average.\r\n",
    "combine_df.ewm(span=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6bf14",
   "metadata": {},
   "source": [
    "# Sharpe Ratios\n",
    "In reality, investment managers and thier institutional investors look at the ratio of return-to-risk, and not just returns alone. After all, if you could invest in one of two portfolios, and each offered the same 10% return, yet one offered lower risk, you'd take that one, right?\n",
    "\n",
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fcf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized Sharpe Ratios\r\n",
    "sharp=combine_df.mean()/combine_df.std()\r\n",
    "sharp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbdaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\r\n",
    "sharp.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c9b68",
   "metadata": {},
   "source": [
    "### Determine whether the algorithmic strategies outperform both the market (S&P 500) and the whales portfolios.\r\n",
    "\r\n",
    "Write your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3fbc85",
   "metadata": {},
   "source": [
    "# Create Custom Portfolio\n",
    "\n",
    "In this section, you will build your own portfolio of stocks, calculate the returns, and compare the results to the Whale Portfolios and the S&P 500. \n",
    "\n",
    "1. Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock.\n",
    "2. Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock\n",
    "3. Join your portfolio returns to the DataFrame that contains all of the portfolio returns\n",
    "4. Re-run the performance and risk analysis with your portfolio to see how it compares to the others\n",
    "5. Include correlation analysis to determine which stocks (if any) are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d0813",
   "metadata": {},
   "source": [
    "## Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock.\n",
    "\n",
    "For this demo solution, we fetch data from three companies listes in the S&P 500 index.\n",
    "\n",
    "* `GOOG` - [Google, LLC](https://en.wikipedia.org/wiki/Google)\n",
    "\n",
    "* `AAPL` - [Apple Inc.](https://en.wikipedia.org/wiki/Apple_Inc.)\n",
    "\n",
    "* `COST` - [Costco Wholesale Corporation](https://en.wikipedia.org/wiki/Costco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first stock\r\n",
    "goog_hist_csv = Path(\"../resources/goog_historical.csv\")\r\n",
    "goog_hist_df = pd.read_csv(goog_hist_csv, index_col = \"Date\", infer_datetime_format=True, parse_dates=True).sort_index(ascending=True)\r\n",
    "goog_hist_df = goog_hist_df.rename(columns = {\"Close\":\"Google\"})\r\n",
    "goog_hist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the second stock\r\n",
    "cost_returns_csv = Path(\"../resources/cost_historical.csv\")\r\n",
    "cost_df = pd.read_csv(ford_returns_csv, index_col = \"Date\", infer_datetime_format=True, parse_dates=True).sort_index(ascending=True)\r\n",
    "cost_df = cost_df.rename(columns = {\"Close\":\"COSTCO\"})\r\n",
    "ford_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ee6b90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apple_returns_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c4ff00496e33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mappl_hist_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"..resources/appl_historical.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mappl_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapple_returns_csv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mappl_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mappl_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"Close\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"Apple\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mappl_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'apple_returns_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Read the third stock\r\n",
    "appl_hist_csv = Path(\"..resources/appl_historical.csv\")\r\n",
    "appl_df = pd.read_csv(applhistns_csv, index_col = \"Date\", infer_datetime_format=True, parse_dates=True).sort_index(ascending=True)\r\n",
    "appl_df = appl_df.rename(columns = {\"Close\":\"Apple\"})\r\n",
    "appl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a803e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all stocks in a single DataFrame\r\n",
    "#combine_df = pd.concat([google_hist_df, cost_hist_df, appl_df], axis=\"columns\", join=\"inner\")\r\n",
    "column_append_data = pd.concat([goog_hist_df, cost_df, appl_df], axis=\"columns\", join = \"inner\")\r\n",
    "column_append_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\r\n",
    "column_append_data=column_append_data.reset_index()\r\n",
    "column_append_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize portfolio data by having a column per symbol\r\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Nulls\r\n",
    "column_append_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6799df",
   "metadata": {},
   "source": [
    "## Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cdf822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted portfolio returns\r\n",
    "# Set weights\r\n",
    "weights = [1/3, 1/3, 1/3]\r\n",
    "column_append_data[[\"Google\", \"Costco\", \"Apple\"]]/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1eee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your \"Custom\" portfolio to the larger dataframe of fund returns\r\n",
    "all_df = pd.concat([column_append_data,combined_df.reset_index()])\r\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08249f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only compare dates where return data exists for all the stocks (drop NaNs)\r\n",
    "all_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d045bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk\r\n",
    "column_append_data.plot.box(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling\r\n",
    "column_append_data.rolling(window=21).std().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6452ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized Sharpe Ratios\r\n",
    "sharp=column_append_data.mean()/column_append_data.std()\r\n",
    "sharp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafbb160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\r\n",
    "sharp.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation analysis\r\n",
    "column_append_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta\r\n",
    "covariance = column_append_data['Google'].cov(column_append_data['Apple'])\r\n",
    "variance = column_append_data['Google'].var()\r\n",
    "SOROS_beta = covariance / variance\r\n",
    "SOROS_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048dc88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67e013bfd48f15f8a996624d3c20cb2a72477efcfdad0ec5eee23742e6997aaa"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
